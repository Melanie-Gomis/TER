{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16521a6",
   "metadata": {},
   "source": [
    "Ce notebook a √©t√© enti√®rement r√©alis√© par M√©lnaie Gomis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb814a",
   "metadata": {},
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49db7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de gdown si besoin\n",
    "try:\n",
    "    import gdown\n",
    "except ImportError:\n",
    "    %pip install -q gdown\n",
    "    import gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2478c4",
   "metadata": {},
   "source": [
    "# Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4883f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    os.chdir(\"../Donn√©es\")\n",
    "    fichiers_npz = glob.glob('*.npz')\n",
    "    cosmos_files = [f for f in fichiers_npz if \"COSMOS\" in f]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68395bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On d√©finit le nom du dossier\n",
    "dossier_nom = \"redshift\"\n",
    "url = \"https://drive.google.com/drive/folders/1-tQH6rfB1XoF7ml98yqVn7Z2IQwGMUdK?usp=sharing\"\n",
    "\n",
    "# 2. LOGIQUE DE T√âL√âCHARGEMENT\n",
    "if os.path.exists(dossier_nom):\n",
    "    print(f\"‚úÖ Le dossier '{dossier_nom}' est d√©j√† pr√©sent. Pas besoin de ret√©l√©charger !\")\n",
    "else:\n",
    "    print(f\"üì• Dossier introuvable. T√©l√©chargement en cours...\")\n",
    "    gdown.download_folder(url, quiet=True, use_cookies=False)\n",
    "    print(\"‚úÖ T√©l√©chargement termin√© !\")\n",
    "\n",
    "# 3. CR√âATION DE LA LISTE DES FICHIERS (C'est ici que √ßa plantait avant)\n",
    "# On scanne le dossier pour cr√©er la liste 'dossier_local' quoi qu'il arrive\n",
    "pattern = os.path.join(dossier_nom, \"*\") # ex: redshift/*\n",
    "dossier_local = glob.glob(pattern)\n",
    "\n",
    "# 4. AFFICHAGE JOLI\n",
    "print(f\"\\nSucc√®s ! {len(dossier_local)} fichiers trouv√©s dans '{dossier_nom}'.\")\n",
    "print(\"D√©tail des fichiers :\")\n",
    "\n",
    "for chemin in dossier_local:\n",
    "    # On extrait le nom du dossier et le nom du fichier\n",
    "    dossier = os.path.dirname(chemin)   # ex: redshift\n",
    "    fichier = os.path.basename(chemin)  # ex: COSMOS_...npz\n",
    "    \n",
    "    # Ton format demand√©\n",
    "    print(f\"[{dossier}] :\\t {fichier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f06e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers_npz = glob.glob('redshift/*.npz')\n",
    "cosmos_files = [f for f in fichiers_npz if \"COSMOS\" in f]\n",
    "\n",
    "# Listes s√©par√©es pour train et test\n",
    "train_cubes_list = []\n",
    "train_infos_list = []\n",
    "train_flag_list = []\n",
    "\n",
    "test_cubes_list = []\n",
    "test_infos_list = []\n",
    "test_flag_list = []\n",
    "\n",
    "print(f\"Chargement de {len(cosmos_files)} fichiers COSMOS.\")\n",
    "\n",
    "for fichier in cosmos_files:\n",
    "    try:\n",
    "        data = np.load(fichier, allow_pickle=True)\n",
    "        \n",
    "        # 'info' est un array structur√©. 'dtype.names' donne les noms des colonnes.\n",
    "        info_fields = data['info'].dtype.names\n",
    "        \n",
    "        # On v√©rifie si la *colonne* 'ZSPEC' existe dans ce fichier\n",
    "        if 'ZSPEC' in info_fields:\n",
    "            # Ce fichier contient des donn√©es de TEST (il a la colonne ZSPEC)\n",
    "            test_cubes_list.append(data['cube'])\n",
    "            test_infos_list.append(data['info'])\n",
    "            test_flag_list.append(data['flag'])\n",
    "        else:\n",
    "            # Ce fichier contient des donn√©es de TRAIN (pas de colonne ZSPEC)\n",
    "            train_cubes_list.append(data['cube'])\n",
    "            train_infos_list.append(data['info'])\n",
    "            train_flag_list.append(data['flag'])\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\":x: Erreur en chargeant {fichier}: {e}\")\n",
    "\n",
    "print(\"Chargement termin√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe114a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation des jeux train/test\n",
    "\n",
    "# Jeu d‚Äôentra√Ænement (bas√© sur les fichiers SANS ZSPEC)\n",
    "# On v√©rifie qu'on a bien trouv√© des fichiers de train\n",
    "if train_infos_list:\n",
    "    X_train = np.concatenate(train_cubes_list, axis=0)\n",
    "    infos_train = np.concatenate(train_infos_list, axis=0)\n",
    "    flags_train = np.concatenate(train_flag_list, axis=0)\n",
    "    y_train = infos_train['ZPHOT'] # Obtenir ZPHOT depuis les infos de train\n",
    "    print(f\"Objets d'entra√Ænement (ZPHOT) : {len(y_train)}\")\n",
    "    print(\"X_train :\", X_train.shape)\n",
    "    print(\"y_train :\", y_train.shape)\n",
    "else:\n",
    "    print(\"Aucun fichier d'entra√Ænement (sans ZSPEC) trouv√©.\")\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# Jeu de test (bas√© sur les fichiers AVEC ZSPEC)\n",
    "# On v√©rifie qu'on a bien trouv√© des fichiers de test\n",
    "if test_infos_list:\n",
    "    X_test = np.concatenate(test_cubes_list, axis=0)\n",
    "    infos_test = np.concatenate(test_infos_list, axis=0)\n",
    "    flags_test = np.concatenate(test_flag_list, axis=0)\n",
    "    y_test = infos_test['ZSPEC'] # Obtenir ZSPEC depuis les infos de test\n",
    "\n",
    "    # Si on ne garde que les ZSPEC valides\n",
    "    # mask_zspec_valid = ~np.isnan(y_test) ou ???\n",
    "    # X_test = X_test[mask_zspec_valid]\n",
    "    # y_test = y_test[mask_zspec_valid]\n",
    "\n",
    "    print(f\"Objets de test (ZSPEC) : {len(y_test)}\")\n",
    "    print(\"X_test  :\", X_test.shape)\n",
    "    print(\"y_test  :\", y_test.shape)\n",
    "else:\n",
    "    print(\"Aucun fichier de test (avec ZSPEC) trouv√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f95cc",
   "metadata": {},
   "source": [
    "# Encodeur d‚Äôimages (Image Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1ceeb",
   "metadata": {},
   "source": [
    "On veut des patchs pr√©cis mais pas trop lourd. \n",
    "\n",
    "- Patch embedding \n",
    "    - On utilisera des patchs de 16x16 feront l'affaire.\n",
    "    - La projection se fera dans un embedding dimension de 512.\n",
    "- Transformers \n",
    "    - On prendre 7 couches et 8 t√™tes (car 512 / 64 = 8).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c196eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(np.ascontiguousarray(X_train), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.ascontiguousarray(y_train), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(np.ascontiguousarray(X_test), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(np.ascontiguousarray(y_test), dtype=torch.float32).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad03055",
   "metadata": {},
   "source": [
    "# Mod√®le sur-mesure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Patch Embedding ---\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size=16, in_channels=9, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        patches = x.unfold(1, self.patch_size, self.patch_size) \\\n",
    "                   .unfold(2, self.patch_size, self.patch_size)\n",
    "        patches = patches.contiguous().view(B, -1, self.patch_size * self.patch_size*C)\n",
    "        patches = self.proj(patches)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, patches), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        return x\n",
    "\n",
    "# --- Transformer Block ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, mlp_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ = self.ln1(x)\n",
    "        x_attn, _ = self.attn(x_, x_, x_)\n",
    "        x = x + x_attn\n",
    "        x = x + self.dropout1(x_attn)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# --- ViT Image Encoder ---\n",
    "class ViTImageEncoder(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size=16, in_channels=9, embed_dim=512,\n",
    "                 depth=7, num_heads=8, mlp_dim=2048):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim) for _ in range(depth)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.permute(1, 0, 2)  # seq_len, batch, embed_dim\n",
    "        for blk in self.transformer_blocks:\n",
    "            x = blk(x)\n",
    "        x = x.permute(1, 0, 2)  # batch, seq_len, embed_dim\n",
    "        x = self.ln(x)\n",
    "        cls_embedding = x[:, 0, :]\n",
    "        return cls_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1913a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" class ParamMLP(nn.Module): \n",
    "    def __init__(self, input_dim=1, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca19ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTRegressor(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size=16, in_channels=9,\n",
    "                 embed_dim=512, depth=7, num_heads=8, mlp_dim=2048):\n",
    "        super().__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "        # ViT encodeur\n",
    "        self.vit = ViTImageEncoder(img_size, patch_size, in_channels,\n",
    "                                   embed_dim, depth, num_heads, mlp_dim)\n",
    "        \n",
    "        # couche finale pour pr√©dire ZPHOT\n",
    "        self.regressor = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.vit(x)  # B, 512\n",
    "        zphot_pred = self.regressor(embedding)  # B,1\n",
    "        return zphot_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea51080",
   "metadata": {},
   "source": [
    "## R√©gression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ViTTrainer:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    # Entra√Ænement (LOSS SEULEMENT)\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            preds = self.model(xb)\n",
    "            loss = self.criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        return epoch_loss\n",
    "\n",
    "    # √âvaluation G√âN√âRIQUE (train OU test)\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                preds = self.model(xb)\n",
    "\n",
    "                loss = self.criterion(preds, yb)\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_targets.append(yb.cpu())\n",
    "\n",
    "        loss = running_loss / len(loader.dataset)\n",
    "\n",
    "        z_pred = torch.cat(all_preds).squeeze().numpy()\n",
    "        z_true = torch.cat(all_targets).squeeze().numpy()\n",
    "\n",
    "        delta_z = z_pred - z_true\n",
    "        delta_z_norm = delta_z / (1.0 + z_true)\n",
    "\n",
    "        biais = np.mean(delta_z_norm)\n",
    "        sigma_nmad = 1.48 * np.median(\n",
    "            np.abs(delta_z_norm - np.median(delta_z_norm))\n",
    "        )\n",
    "\n",
    "        return loss, biais, sigma_nmad\n",
    "\n",
    "    # Boucle principale\n",
    "    def fit(self, train_loader, test_loader, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "\n",
    "            train_loss_eval, train_biais, train_sigma = self.evaluate(train_loader)\n",
    "            test_loss, test_biais, test_sigma = self.evaluate(test_loader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss : {train_loss:.4f}\")\n",
    "            print(f\"Test  Loss : {test_loss:.4f}\")\n",
    "            print(f\"Train Biais : {train_biais:.5f} | œÉ_NMAD : {train_sigma:.5f}\")\n",
    "            print(f\"Test  Biais : {test_biais:.5f} | œÉ_NMAD : {test_sigma:.5f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c3340",
   "metadata": {},
   "source": [
    "Une √©poque prends environs 13.25 secondes avec les GPU T4 de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200 # augmenter si n√©cessaire\n",
    "tps = 13.25\n",
    "tps_approx = tps * num_epochs\n",
    "\n",
    "\n",
    "# Conversion des secondes en jours, heures et minutes\n",
    "jours = tps_approx // (24 * 3600)\n",
    "tps_approx %= (24 * 3600)\n",
    "heures = tps_approx // 3600\n",
    "tps_approx %= 3600\n",
    "minutes = tps_approx // 60\n",
    "secondes = tps_approx % 60\n",
    "\n",
    "\n",
    "# Affichage\n",
    "print(f\"Temps approximatif : {int(jours)} J {int(heures)} H {int(minutes)} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ViTRegressor().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5719e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ViTTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "trainer.fit(train_loader, test_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = trainer.train_losses\n",
    "test_losses  = trainer.test_losses\n",
    "train_biases = trainer.train_biases\n",
    "test_biases  = trainer.test_biases\n",
    "train_sigma  = trainer.train_sigma\n",
    "test_sigma   = trainer.test_sigma\n",
    "\n",
    "epochs = range(1, len(train_losses)+1)\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
    "\n",
    "# --------- 1. Loss ----------\n",
    "axs[0,0].plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "axs[0,0].plot(epochs, test_losses, label='Test Loss', marker='o')\n",
    "axs[0,0].set_xlabel('Epoch')\n",
    "axs[0,0].set_ylabel('Loss')\n",
    "axs[0,0].set_title('Loss')\n",
    "axs[0,0].grid(True)\n",
    "axs[0,0].legend()\n",
    "\n",
    "# --------- 2. œÉ_NMAD ----------\n",
    "axs[0,1].plot(epochs, train_sigma, label='Train œÉ_NMAD', marker='o')\n",
    "axs[0,1].plot(epochs, test_sigma, label='Test œÉ_NMAD', marker='o')\n",
    "axs[0,1].axhline(0.014, color='red', linestyle='--', label='Target')\n",
    "axs[0,1].set_xlabel('Epoch')\n",
    "axs[0,1].set_ylabel('œÉ_NMAD')\n",
    "axs[0,1].set_title('œÉ_NMAD')\n",
    "axs[0,1].grid(True)\n",
    "axs[0,1].legend()\n",
    "\n",
    "# --------- 3. Bias ----------\n",
    "axs[1,0].plot(epochs, train_biases, label='Train Bias', marker='o')\n",
    "axs[1,0].plot(epochs, test_biases, label='Test Bias', marker='o')\n",
    "axs[1,0].axhline(0, color='red', linestyle='--', label='Target')\n",
    "axs[1,0].set_xlabel('Epoch')\n",
    "axs[1,0].set_ylabel('Bias')\n",
    "axs[1,0].set_title('Bias')\n",
    "axs[1,0].grid(True)\n",
    "axs[1,0].legend()\n",
    "\n",
    "# --------- 4. Encadr√© r√©sum√© ----------\n",
    "axs[1,1].axis('off')  # pas d'axes\n",
    "\n",
    "# Texte r√©sum√© avec les valeurs finales\n",
    "textstr = (\n",
    "    f\"Final Train:\\n\"\n",
    "    f\"Loss  = {train_losses[-1]:.4f}\\n\"\n",
    "    f\"œÉ_NMAD = {train_sigma[-1]:.4f}\\n\"\n",
    "    f\"Bias   = {train_biases[-1]:.4f}\\n\\n\"\n",
    "    f\"Final Test:\\n\"\n",
    "    f\"Loss  = {test_losses[-1]:.4f}\\n\"\n",
    "    f\"œÉ_NMAD = {test_sigma[-1]:.4f}\\n\"\n",
    "    f\"Bias   = {test_biases[-1]:.4f}\"\n",
    ")\n",
    "axs[1,1].text(0.5, 0.5, textstr, fontsize=12, ha='center', va='center',\n",
    "              bbox=dict(facecolor='lightgray', edgecolor='black', boxstyle='round'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d3faf",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02680ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classe d'entra√Ænement pour le mod√®le ViT ---\n",
    "class ViTTrainerClassification:\n",
    "    def __init__(self, num_classes=200, lr=1e-4):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = ViTModel(num_classes=num_classes).to(self.device)\n",
    "        \n",
    "        # D√©finir le crit√®re de perte et l'optimiseur\n",
    "        self.criterion = nn.CrossEntropyLoss()  # Utilisation de la CrossEntropyLoss pour la classification\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, train_loader, num_epochs=10):\n",
    "        self.model.train()  # Mettre le mod√®le en mode entra√Ænement\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()  # Z√©roiser les gradients\n",
    "                \n",
    "                outputs = self.model(inputs)  # Passer les entr√©es dans le mod√®le\n",
    "                loss = self.criterion(outputs, labels)  # Calculer la perte\n",
    "                \n",
    "                loss.backward()  # Calculer les gradients\n",
    "                self.optimizer.step()  # Mettre √† jour les poids\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()  # Mettre le mod√®le en mode √©valuation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():  # Ne pas calculer les gradients pour l'√©valuation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)  # Obtenir la classe avec la probabilit√© la plus √©lev√©e\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = correct / total * 100\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca18949",
   "metadata": {},
   "source": [
    "# Mod√®le pr√©-fait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40368439",
   "metadata": {},
   "source": [
    "Au lieu de faire une r√©gression, on va faire une classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe pour le mod√®le ViT\n",
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, num_classes=200):  # Param√®tre pour sp√©cifier le nombre de classes\n",
    "        super(ViTModel, self).__init__()\n",
    "        # Exemple d'utilisation d'un mod√®le ViT pr√©-entrain√© (par ex., de Hugging Face ou PyTorch)\n",
    "        # Ici, on suppose l'utilisation d'un mod√®le pr√©existant, comme ViT-B/16 de Hugging Face\n",
    "        self.vit = models.vit_b_16(pretrained=True)  # Charger le mod√®le ViT pr√©-entrain√©\n",
    "        self.vit.conv_proj = nn.Conv2d(\n",
    "            9,                 # üî• nombre de canaux d'entr√©e\n",
    "            768,               # embed dim du ViT-B/16\n",
    "            kernel_size=16,\n",
    "            stride=16\n",
    "        )\n",
    "\n",
    "        # Remplacer la couche de classification pour correspondre au nombre de classes\n",
    "        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "# Classe pour entra√Æner et g√©rer le mod√®le\n",
    "class ViTTrainer:\n",
    "    def __init__(self, num_classes=200, lr=1e-4):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = ViTModel(num_classes=num_classes).to(self.device)\n",
    "        \n",
    "        # D√©finir le crit√®re de perte et l'optimiseur\n",
    "        self.criterion = nn.CrossEntropyLoss()  # Utilisation de la CrossEntropyLoss pour la classification\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "    def train(self, train_loader, num_epochs=10):\n",
    "        self.model.train()  # Mettre le mod√®le en mode entra√Ænement\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()  # Z√©roiser les gradients\n",
    "                \n",
    "                outputs = self.model(inputs)  # Passer les entr√©es dans le mod√®le\n",
    "                loss = self.criterion(outputs, labels)  # Calculer la perte\n",
    "                \n",
    "                loss.backward()  # Calculer les gradients\n",
    "                self.optimizer.step()  # Mettre √† jour les poids\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()  # Mettre le mod√®le en mode √©valuation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():  # Ne pas calculer les gradients pour l'√©valuation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)  # Obtenir la classe avec la probabilit√© la plus √©lev√©e\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = correct / total * 100\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # √©valuation rapide sur test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            test_loss += criterion(pred, yb).item() * xb.size(0)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Courbe de Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
