{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bb990d",
   "metadata": {},
   "source": [
    "Ce notebook a √©t√© enti√®rement r√©alis√© par M√©lnaie Gomis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7e7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563dced3",
   "metadata": {},
   "source": [
    "# Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e073a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Le dossier 'redshift' est d√©j√† pr√©sent. Pas besoin de ret√©l√©charger !\n",
      "\n",
      "Succ√®s ! 8 fichiers trouv√©s dans 'redshift'.\n",
      "D√©tail des fichiers :\n",
      "[redshift] :\t COSMOS_v11_uijk_0213_photo_UD.npz\n",
      "[redshift] :\t XMM_LSS_v11_uijk_0162_phot_UD.npz\n",
      "[redshift] :\t XMM_LSS_v11_uijk_0006_spec_D.npz\n",
      "[redshift] :\t XMM_LSS_v11_uijk_0177_spec_UD.npz\n",
      "[redshift] :\t COSMOS_v11_uijk_0020_spec_D.npz\n",
      "[redshift] :\t COSMOS_v11_uijk_0073_spec_UD.npz\n",
      "[redshift] :\t COSMOS_v11_uijk_0001_photo_D.npz\n",
      "[redshift] :\t XMM_LSS_v11_uijk_0162_phot_D.npz\n"
     ]
    }
   ],
   "source": [
    "# On d√©finit le nom du dossier\n",
    "dossier_nom = \"redshift\"\n",
    "url = \"https://drive.google.com/drive/folders/1-tQH6rfB1XoF7ml98yqVn7Z2IQwGMUdK\"\n",
    "\n",
    "# 2. LOGIQUE DE T√âL√âCHARGEMENT\n",
    "if os.path.exists(dossier_nom):\n",
    "    print(f\"‚úÖ Le dossier '{dossier_nom}' est d√©j√† pr√©sent. Pas besoin de ret√©l√©charger !\")\n",
    "else:\n",
    "    print(f\"üì• Dossier introuvable. T√©l√©chargement en cours...\")\n",
    "    gdown.download_folder(url, quiet=True, use_cookies=False)\n",
    "    print(\"‚úÖ T√©l√©chargement termin√© !\")\n",
    "\n",
    "# 3. CR√âATION DE LA LISTE DES FICHIERS (C'est ici que √ßa plantait avant)\n",
    "# On scanne le dossier pour cr√©er la liste 'dossier_local' quoi qu'il arrive\n",
    "pattern = os.path.join(dossier_nom, \"*.npz\") # ex: redshift/*\n",
    "dossier_local = glob.glob(pattern)\n",
    "\n",
    "# 4. AFFICHAGE JOLI\n",
    "print(f\"\\nSucc√®s ! {len(dossier_local)} fichiers trouv√©s dans '{dossier_nom}'.\")\n",
    "print(\"D√©tail des fichiers :\")\n",
    "\n",
    "for chemin in dossier_local:\n",
    "    # On extrait le nom du dossier et le nom du fichier\n",
    "    dossier = os.path.dirname(chemin)   # ex: redshift\n",
    "    fichier = os.path.basename(chemin)  # ex: COSMOS_...npz\n",
    "    \n",
    "    # Ton format demand√©\n",
    "    print(f\"[{dossier}] :\\t {fichier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf3443d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de 4 fichiers COSMOS.\n",
      "Chargement termin√©.\n"
     ]
    }
   ],
   "source": [
    "fichiers_npz = glob.glob('redshift/*.npz')\n",
    "cosmos_files = [f for f in fichiers_npz if \"COSMOS\" in f]\n",
    "\n",
    "# Listes s√©par√©es pour train et test\n",
    "train_cubes_list = []\n",
    "train_infos_list = []\n",
    "train_flag_list = []\n",
    "\n",
    "test_cubes_list = []\n",
    "test_infos_list = []\n",
    "test_flag_list = []\n",
    "\n",
    "print(f\"Chargement de {len(cosmos_files)} fichiers COSMOS.\")\n",
    "\n",
    "for fichier in cosmos_files:\n",
    "    try:\n",
    "        data = np.load(fichier, allow_pickle=True)\n",
    "        \n",
    "        # 'info' est un array structur√©. 'dtype.names' donne les noms des colonnes.\n",
    "        info_fields = data['info'].dtype.names\n",
    "        \n",
    "        # On v√©rifie si la *colonne* 'ZSPEC' existe dans ce fichier\n",
    "        if 'ZSPEC' in info_fields:\n",
    "            # Ce fichier contient des donn√©es de TEST (il a la colonne ZSPEC)\n",
    "            test_cubes_list.append(data['cube'])\n",
    "            test_infos_list.append(data['info'])\n",
    "            test_flag_list.append(data['flag'])\n",
    "        else:\n",
    "            # Ce fichier contient des donn√©es de TRAIN (pas de colonne ZSPEC)\n",
    "            train_cubes_list.append(data['cube'])\n",
    "            train_infos_list.append(data['info'])\n",
    "            train_flag_list.append(data['flag'])\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\":x: Erreur en chargeant {fichier}: {e}\")\n",
    "\n",
    "print(\"Chargement termin√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070f4060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objets d'entra√Ænement (ZPHOT) : 12497\n",
      "X_train : (12497, 64, 64, 9)\n",
      "y_train : (12497,)\n",
      "\n",
      "---\n",
      "\n",
      "Objets de test (ZSPEC) : 27\n",
      "X_test  : (27, 64, 64, 9)\n",
      "y_test  : (27,)\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation des jeux train/test\n",
    "\n",
    "# Jeu d‚Äôentra√Ænement (bas√© sur les fichiers SANS ZSPEC)\n",
    "# On v√©rifie qu'on a bien trouv√© des fichiers de train\n",
    "if train_infos_list:\n",
    "    X_train = np.concatenate(train_cubes_list, axis=0)\n",
    "    infos_train = np.concatenate(train_infos_list, axis=0)\n",
    "    flags_train = np.concatenate(train_flag_list, axis=0)\n",
    "    y_train = infos_train['ZPHOT'] # Obtenir ZPHOT depuis les infos de train\n",
    "    print(f\"Objets d'entra√Ænement (ZPHOT) : {len(y_train)}\")\n",
    "    print(\"X_train :\", X_train.shape)\n",
    "    print(\"y_train :\", y_train.shape)\n",
    "else:\n",
    "    print(\"Aucun fichier d'entra√Ænement (sans ZSPEC) trouv√©.\")\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# Jeu de test (bas√© sur les fichiers AVEC ZSPEC)\n",
    "# On v√©rifie qu'on a bien trouv√© des fichiers de test\n",
    "if test_infos_list:\n",
    "    X_test = np.concatenate(test_cubes_list, axis=0)\n",
    "    infos_test = np.concatenate(test_infos_list, axis=0)\n",
    "    flags_test = np.concatenate(test_flag_list, axis=0)\n",
    "    y_test = infos_test['ZSPEC'] # Obtenir ZSPEC depuis les infos de test\n",
    "\n",
    "    # Si on ne garde que les ZSPEC valides\n",
    "    # mask_zspec_valid = ~np.isnan(y_test) ou ???\n",
    "    # X_test = X_test[mask_zspec_valid]\n",
    "    # y_test = y_test[mask_zspec_valid]\n",
    "\n",
    "    print(f\"Objets de test (ZSPEC) : {len(y_test)}\")\n",
    "    print(\"X_test  :\", X_test.shape)\n",
    "    print(\"y_test  :\", y_test.shape)\n",
    "else:\n",
    "    print(\"Aucun fichier de test (avec ZSPEC) trouv√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada3a42",
   "metadata": {},
   "source": [
    "# Chargement des codes python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53be974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Le dossier 'redshift/Marie Treyer' est d√©j√† pr√©sent. Pas besoin de ret√©l√©charger !\n",
      "\n",
      "Succ√®s ! 9 fichiers .py trouv√©s dans 'redshift/Marie Treyer'.\n",
      "\n",
      "[redshift/Marie Treyer] :\t models_building_blocks.py\n",
      "[redshift/Marie Treyer] :\t model_multi_modal_simple.py\n",
      "[redshift/Marie Treyer] :\t __pycache__\n",
      "[redshift/Marie Treyer] :\t utils\n",
      "[redshift/Marie Treyer/__pycache__] :\t model_multi_modal_simple.cpython-312.pyc\n",
      "[redshift/Marie Treyer/__pycache__] :\t models_building_blocks.cpython-312.pyc\n",
      "[redshift/Marie Treyer/utils] :\t config.py\n",
      "[redshift/Marie Treyer/utils] :\t __pycache__\n",
      "[redshift/Marie Treyer/utils/__pycache__] :\t config.cpython-312.pyc\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Nom du dossier local\n",
    "dossier_nom = \"redshift/Marie Treyer\"\n",
    "\n",
    "# 2Ô∏è‚É£ ID du dossier Google Drive\n",
    "folder_id = \"1mqIUBkZ3qN82hQ3H5WEdz0YYggMxZe3J\"\n",
    "\n",
    "# 3Ô∏è‚É£ T√©l√©chargement si n√©cessaire\n",
    "if os.path.exists(dossier_nom):\n",
    "    print(f\"‚úÖ Le dossier '{dossier_nom}' est d√©j√† pr√©sent. Pas besoin de ret√©l√©charger !\")\n",
    "else:\n",
    "    print(f\"üì• Dossier introuvable. T√©l√©chargement en cours...\")\n",
    "    try:\n",
    "        gdown.download_folder(\n",
    "            id=folder_id,       # utilisation de l'ID\n",
    "            output=dossier_nom,\n",
    "            quiet=False,\n",
    "            use_cookies=False\n",
    "        )\n",
    "        print(\"‚úÖ T√©l√©chargement termin√© !\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Impossible de t√©l√©charger le dossier avec gdown.\")\n",
    "        print(\"Essayez de le t√©l√©charger manuellement depuis le navigateur.\")\n",
    "        raise e\n",
    "\n",
    "# 4Ô∏è‚É£ Recherche r√©cursive de tous les fichiers .py\n",
    "pattern = os.path.join(dossier_nom, \"**\", \"*.py\")\n",
    "fichiers_py = glob.glob(pattern, recursive=True)\n",
    "\n",
    "# 5Ô∏è‚É£ Affichage\n",
    "print(f\"\\nSucc√®s ! {len(fichiers_py)} fichiers .py trouv√©s dans '{dossier_nom}'.\\n\")\n",
    "for chemin in fichiers_py:\n",
    "    dossier = os.path.dirname(chemin)\n",
    "    fichier = os.path.basename(chemin)\n",
    "    print(f\"[{dossier}] :\\t {fichier}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43591012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin absolu du dossier contenant les modules\n",
    "dossier_nom = \"redshift/Marie Treyer\"\n",
    "chemin_absolu = os.path.abspath(dossier_nom)\n",
    "\n",
    "if chemin_absolu not in sys.path:\n",
    "    sys.path.append(chemin_absolu)\n",
    "\n",
    "# Maintenant l'import devrait fonctionner\n",
    "from model_multi_modal_simple import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aaf746",
   "metadata": {},
   "source": [
    "# Cr√©ation du mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94925198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.in_dim [64, 64]\n",
      "self.modalities 1\n",
      "\n",
      "-- Lancement de la pr√©diction --\n",
      "Dimension sortie R√©gression : torch.Size([5, 1])\n",
      "Valeurs pr√©dites par le mod√®le (non entra√Æn√©) :\n",
      "[[0.33071488]\n",
      " [0.3312552 ]\n",
      " [0.33327168]\n",
      " [0.33254465]\n",
      " [0.33317596]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from model_multi_modal_simple import Model_multi_modal_simple\n",
    "import utils.config as config\n",
    "\n",
    "# 1. Pr√©paration des tenseurs\n",
    "X_train_tensor = torch.tensor(np.transpose(X_train, (0, 3, 1, 2)).copy(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(np.transpose(X_test, (0, 3, 1, 2)).copy(), dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.copy(), dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test.copy(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 2. Instanciation du mod√®le\n",
    "modalities = [list(range(9))]\n",
    "\n",
    "model = Model_multi_modal_simple(\n",
    "    in_dim=[64, 64],\n",
    "    n_outputs=1, \n",
    "    modalities=modalities,\n",
    "    mags_input_size=None, \n",
    "    parallel_before_inception_archi=[32, 32],\n",
    "    parallel_inception_archi=[36, 32, 32, 32, 42, 42], \n",
    "    parallel_pooling_before_inceptions=[True, False, False, False, False, False], \n",
    "    inception_archi=[109, 101, 101, 101, 156, 156], \n",
    "    pooling_before_inceptions=[False, True, False, True, False, False] \n",
    ")\n",
    "\n",
    "device = torch.device(config.CONFIG[\"DEVICE\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 3. Lancement du Forward Pass (avec le correctif)\n",
    "print(\"\\n-- Lancement de la pr√©diction --\")\n",
    "with torch.no_grad(): \n",
    "    batch_X = X_test_tensor[:5].to(device)\n",
    "    \n",
    "    # CORRECTIF : Cr√©ation d'un tenseur \"ebv\" factice (des z√©ros) de la m√™me taille que le batch\n",
    "    # Cela permet de contourner le bug d'aplatissement et comble le \"+1\" attendu par la couche FC\n",
    "    dummy_ebv = torch.zeros(batch_X.size(0)).to(device)\n",
    "    \n",
    "    # On passe notre variable dummy_ebv au mod√®le\n",
    "    out_classif, out_regress = model(batch_X, ebv=dummy_ebv)\n",
    "    \n",
    "    print(f\"Dimension sortie R√©gression : {out_regress.shape}\")\n",
    "    print(f\"Valeurs pr√©dites par le mod√®le (non entra√Æn√©) :\\n{out_regress.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e25ec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Pr√©paration des donn√©es d'entra√Ænement --\n",
      "\n",
      "-- D√©but de l'entra√Ænement sur cpu --\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import utils.config as config\n",
    "\n",
    "# --- CORRECTIF DROPOUT ---\n",
    "# On d√©sactive le dropout de modalit√© puisqu'on a regroup√© les 9 canaux dans 1 seule modalit√©\n",
    "config.CONFIG[\"MODALITY_DROP_OUT\"] = None\n",
    "\n",
    "# 1. Cr√©ation du jeu de donn√©es et du DataLoader\n",
    "print(\"-- Pr√©paration des donn√©es d'entra√Ænement --\")\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.CONFIG[\"BATCH_SIZE\"], \n",
    "    shuffle=True, \n",
    "    num_workers=config.CONFIG.get(\"NUM_WORKERS\", 0), \n",
    "    pin_memory=config.CONFIG.get(\"PIN_MEMORY\", False) \n",
    ")\n",
    "\n",
    "# 2. D√©finition de la fonction de Perte et de l'Optimiseur\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Param√®tres de la boucle d'entra√Ænement\n",
    "epochs = 10 \n",
    "device = torch.device(config.CONFIG[\"DEVICE\"]) \n",
    "\n",
    "print(f\"\\n-- D√©but de l'entra√Ænement sur {device} --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e656b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Mode entra√Ænement (active les Batch Norms)\n",
    "    model.train() \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Faux tenseur ebv (z√©ro) pour ce batch sp√©cifique\n",
    "        dummy_ebv = torch.zeros(batch_X.size(0)).to(device)\n",
    "        \n",
    "        # √âtape 1 : R√©initialiser les gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # √âtape 2 : Pr√©diction\n",
    "        out_classif, out_regress = model(batch_X, ebv=dummy_ebv)\n",
    "        \n",
    "        # √âtape 3 : Calcul de l'erreur\n",
    "        loss = criterion(out_regress, batch_y)\n",
    "        \n",
    "        # √âtape 4 : R√©tropropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # √âtape 5 : Mise √† jour des poids\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistiques\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Affichage r√©gulier\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"√âpoque [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Perte (MSE): {running_loss/100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    print(f\"==> Fin de l'√©poque {epoch+1} termin√©e. <==\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
